{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0191c29c-7f7e-4780-b870-fff289599da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a0cb655-603b-4038-b5f4-4ecd67f4c801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 2])\n",
      "tensor([[[ 1.,  2.],\n",
      "         [ 3.,  4.],\n",
      "         [ 5.,  6.],\n",
      "         [ 7.,  8.],\n",
      "         [ 9., 10.]]])\n",
      "torch.Size([1, 3, 1])\n",
      "tensor([[[ 0.5859],\n",
      "         [-4.1594],\n",
      "         [ 2.1191]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# x = torch.randn(1, 5, 2)\n",
    "x = torch.from_numpy(np.array([[[1,2], [3,4], [5,6], [7, 8], [9, 10]]])).float()\n",
    "print(x.shape)\n",
    "print(x)\n",
    "\n",
    "conv1d = nn.Conv1d(in_channels=5, out_channels=3, kernel_size=2)\n",
    "\n",
    "output = conv1d(x)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dbbccff-0460-4240-87ca-753e5b033fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.,  2.],\n",
      "         [ 3.,  4.],\n",
      "         [ 5.,  6.],\n",
      "         [ 7.,  8.],\n",
      "         [ 9., 10.]]])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4b010f8-18d7-4c1e-bf03-09cee86baf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.5859],\n",
      "         [-4.1594],\n",
      "         [ 2.1191]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94b66fd4-c06a-4c80-94bc-5d43d81effde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences with mocap data: 152\n",
      "Number of sequences with mocap data and parallel cameras: 76\n",
      "Number of sequences with mocap data, parallel cameras and after clothing change: 12\n",
      "Number of unique participants with mocap data and parallel cameras: 32\n",
      "Number of unique participants with mocap data, parallel cameras and after clothing change: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'p26', 'p27', 'p28', 'p29', 'p30', 'p31'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scripts.parsers import parse_sequences as parse_sequence_info\n",
    "\n",
    "file_path = 'gait3d\\\\ListOfSequences.txt'\n",
    "sequences = parse_sequence_info(file_path)\n",
    "\n",
    "mocap_keys = []\n",
    "par_cam_keys = []\n",
    "par_cam_person = set()\n",
    "par_after_cloth_change_keys = []\n",
    "par_after_cloth_change_person = set()\n",
    "\n",
    "for key, params in sequences.items():\n",
    "    if params['MoCap_data']:\n",
    "        mocap_keys.append(key)\n",
    "        if key[-1] in [\"1\", \"3\", \"5\", \"7\"]:\n",
    "            par_cam_keys.append(key)\n",
    "            par_cam_person.add(key[:-2])\n",
    "        if key[-1] in [\"5\", \"7\"]:\n",
    "            par_after_cloth_change_keys.append(key)\n",
    "            par_after_cloth_change_person.add(key[:-2])\n",
    "\n",
    "print(f\"Number of sequences with mocap data: {len(mocap_keys)}\")\n",
    "print(f\"Number of sequences with mocap data and parallel cameras: {len(par_cam_keys)}\")\n",
    "print(f\"Number of sequences with mocap data, parallel cameras and after clothing change: {len(par_after_cloth_change_keys)}\")\n",
    "print(f\"Number of unique participants with mocap data and parallel cameras: {len(par_cam_person)}\")\n",
    "print(f\"Number of unique participants with mocap data, parallel cameras and after clothing change: {len(par_after_cloth_change_person)}\")\n",
    "par_after_cloth_change_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3395950f-6678-499e-a788-0d60ec5327ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test sequences: ['p13s1', 'p13s3', 'p14s1', 'p14s3', 'p32s1', 'p32s3', 'p28s5', 'p28s7', 'p31s5', 'p31s7']\n",
      "valid sequences: ['p24s1', 'p24s3', 'p21s1', 'p21s3', 'p1s1', 'p1s3', 'p26s5', 'p26s7', 'p27s5', 'p27s7']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "without_clothing_change = []\n",
    "while len(without_clothing_change) < 6:\n",
    "    random_person = random.choice(list(par_cam_person))\n",
    "    if random_person not in par_after_cloth_change_person:\n",
    "        without_clothing_change.append(random_person)\n",
    "        par_cam_person.remove(random_person)\n",
    "\n",
    "with_clothing_change = []\n",
    "while len(with_clothing_change) < 4:\n",
    "    random_person = random.choice(list(par_after_cloth_change_person))\n",
    "    with_clothing_change.append(random_person)\n",
    "    par_after_cloth_change_person.remove(random_person)\n",
    "\n",
    "\n",
    "test_seq_set = ([f'{p_seq}s{seq_idx}' for p_seq in without_clothing_change[:3] for seq_idx in [1, 3]] +\n",
    "                [f'{p_seq}s{seq_idx}' for p_seq in with_clothing_change[:2] for seq_idx in [5, 7]])\n",
    "\n",
    "valid_seq_set = ([f'{p_seq}s{seq_idx}' for p_seq in without_clothing_change[3:] for seq_idx in [1, 3]] +\n",
    "                [f'{p_seq}s{seq_idx}' for p_seq in with_clothing_change[2:] for seq_idx in [5, 7]])\n",
    "\n",
    "print(f\"test sequences: {test_seq_set}\")\n",
    "print(f\"valid sequences: {valid_seq_set}\")\n",
    "# without_clothing_change + [only_after_clothing_change] + [only_before_clothing_change]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac4b8fb8-e46b-4102-916d-869bc04310e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sequences: ['p15s1', 'p15s3', 'p16s1', 'p16s3', 'p7s1', 'p7s3', 'p26s1', 'p26s3', 'p11s1', 'p11s3', 'p18s1', 'p18s3', 'p29s1', 'p29s3', 'p9s1', 'p9s3', 'p28s1', 'p28s3', 'p6s1', 'p6s3', 'p12s1', 'p12s3', 'p10s1', 'p10s3', 'p20s1', 'p20s3', 'p3s1', 'p3s3', 'p27s1', 'p27s3', 'p22s1', 'p22s3', 'p2s1', 'p2s3', 'p23s1', 'p23s3', 'p30s1', 'p30s3', 'p25s1', 'p25s3', 'p17s1', 'p17s3', 'p31s1', 'p31s3', 'p5s1', 'p5s3', 'p8s1', 'p8s3', 'p19s1', 'p19s3', 'p4s1', 'p4s3', 'p29s5', 'p29s7', 'p30s5', 'p30s7']\n"
     ]
    }
   ],
   "source": [
    "train_seq_set = ([f'{p_seq}s{seq_idx}' for p_seq in list(par_cam_person) for seq_idx in [1, 3]] +\n",
    "                 [f'{p_seq}s{seq_idx}' for p_seq in list(par_after_cloth_change_person) for seq_idx in [5, 7]])\n",
    "\n",
    "print(f\"train sequences: {train_seq_set}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fd1ea23-7065-456b-a8b6-288272c37503",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1s1 |       | valid |      |\n",
      "p1s3 |       | valid |      |\n",
      "p2s1 | train |       |      |\n",
      "p2s3 | train |       |      |\n",
      "p3s1 | train |       |      |\n",
      "p3s3 | train |       |      |\n",
      "p4s1 | train |       |      |\n",
      "p4s3 | train |       |      |\n",
      "p5s1 | train |       |      |\n",
      "p5s3 | train |       |      |\n",
      "p6s1 | train |       |      |\n",
      "p6s3 | train |       |      |\n",
      "p7s1 | train |       |      |\n",
      "p7s3 | train |       |      |\n",
      "p8s1 | train |       |      |\n",
      "p8s3 | train |       |      |\n",
      "p9s1 | train |       |      |\n",
      "p9s3 | train |       |      |\n",
      "p10s1 | train |       |      |\n",
      "p10s3 | train |       |      |\n",
      "p11s1 | train |       |      |\n",
      "p11s3 | train |       |      |\n",
      "p12s1 | train |       |      |\n",
      "p12s3 | train |       |      |\n",
      "p13s1 |       |       | test |\n",
      "p13s3 |       |       | test |\n",
      "p14s1 |       |       | test |\n",
      "p14s3 |       |       | test |\n",
      "p15s1 | train |       |      |\n",
      "p15s3 | train |       |      |\n",
      "p16s1 | train |       |      |\n",
      "p16s3 | train |       |      |\n",
      "p17s1 | train |       |      |\n",
      "p17s3 | train |       |      |\n",
      "p18s1 | train |       |      |\n",
      "p18s3 | train |       |      |\n",
      "p19s1 | train |       |      |\n",
      "p19s3 | train |       |      |\n",
      "p20s1 | train |       |      |\n",
      "p20s3 | train |       |      |\n",
      "p21s1 |       | valid |      |\n",
      "p21s3 |       | valid |      |\n",
      "p22s1 | train |       |      |\n",
      "p22s3 | train |       |      |\n",
      "p23s1 | train |       |      |\n",
      "p23s3 | train |       |      |\n",
      "p24s1 |       | valid |      |\n",
      "p24s3 |       | valid |      |\n",
      "p25s1 | train |       |      |\n",
      "p25s3 | train |       |      |\n",
      "p26s1 | train |       |      |\n",
      "p26s3 | train |       |      |\n",
      "p26s5 |       | valid |      |\n",
      "p26s7 |       | valid |      |\n",
      "p27s1 | train |       |      |\n",
      "p27s3 | train |       |      |\n",
      "p27s5 |       | valid |      |\n",
      "p27s7 |       | valid |      |\n",
      "p28s1 | train |       |      |\n",
      "p28s3 | train |       |      |\n",
      "p28s5 |       |       | test |\n",
      "p28s7 |       |       | test |\n",
      "p29s1 | train |       |      |\n",
      "p29s3 | train |       |      |\n",
      "p29s5 | train |       |      |\n",
      "p29s7 | train |       |      |\n",
      "p30s1 | train |       |      |\n",
      "p30s3 | train |       |      |\n",
      "p30s5 | train |       |      |\n",
      "p30s7 | train |       |      |\n",
      "p31s1 | train |       |      |\n",
      "p31s3 | train |       |      |\n",
      "p31s5 |       |       | test |\n",
      "p31s7 |       |       | test |\n",
      "p32s1 |       |       | test |\n",
      "p32s3 |       |       | test |\n"
     ]
    }
   ],
   "source": [
    "for key, params in sequences.items():\n",
    "    if params['MoCap_data']:\n",
    "        if key[-1] in [\"1\", \"3\", \"5\", \"7\"]:\n",
    "            print(f\"{key} | {'train' if key in train_seq_set else '     '} | {'valid' if key in valid_seq_set else '     '} | {'test' if key in test_seq_set else '    '} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af60bc52-4453-4753-baf2-8650f1b9b70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 56 | 77.78%\n",
      "Test size: 10 | 13.89%\n",
      "Valid size: 10 | 13.89%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train size: {len(train_seq_set)} | {100*len(train_seq_set)/72:.2f}%\")\n",
    "print(f\"Test size: {len(test_seq_set)} | {100*len(test_seq_set)/72:.2f}%\")\n",
    "print(f\"Valid size: {len(valid_seq_set)} | {100*len(valid_seq_set)/72:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a0cbbf8-68b8-4fd5-a614-a293ec8854a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "selected_names_file = \"./datasets/mediapipe/selected_joint_names.json\"\n",
    "input_data_file = \"./datasets/mediapipe/dataset_v2.json\"\n",
    "output_data_file = \"./datasets/mocap/dataset_v2.json\"\n",
    "\n",
    "with open(input_data_file, 'r') as file:\n",
    "    raw_input = json.load(file)\n",
    "\n",
    "with open(output_data_file, 'r') as file:\n",
    "    raw_output = json.load(file)\n",
    "\n",
    "with open(selected_names_file, 'r') as file:\n",
    "    selected_names = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8138529-b136-4a38-aeb8-2fd670c8b52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "triang_data_file = \"./datasets/mediapipe/triangulation.json\"\n",
    "\n",
    "with open(triang_data_file, 'r') as file:\n",
    "    triangulation_data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac3d5644-056f-49ee-8f7a-7576605b1d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start_frame': 195,\n",
       " 'number_of_frames': 135,\n",
       " 'frame_offset': 0,\n",
       " 'MoCap_data': True}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences['p1s1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "575c6d0f-1bf2-4cfd-bb23-06698b0aa8be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames with all found mocaps: 6035\n",
      "Frames with at least one not found mocap: 3495\n",
      "Proportion: 63.33%\n"
     ]
    }
   ],
   "source": [
    "input_frames_data = {f\"c{c_idx}\": [] for c_idx in range(1, 5)}\n",
    "output_frames_data = []\n",
    "img_width = 960\n",
    "img_height = 540\n",
    "\n",
    "not_found = 0\n",
    "seq_keys_list = train_seq_set + test_seq_set + valid_seq_set\n",
    "\n",
    "for seq_key in seq_keys_list:\n",
    "    for f_idx in range(sequences[seq_key]['number_of_frames']):\n",
    "    # for f_idx in range(2):\n",
    "        curr_output_array = []\n",
    "        output_frame_dict = raw_output[seq_key][f_idx]\n",
    "        for point_idx, joint_name in selected_names.items():\n",
    "            curr_output_array.append(output_frame_dict[joint_name])\n",
    "\n",
    "        curr_output_array_np = np.array(curr_output_array)\n",
    "        # print(curr_output_array_np)\n",
    "        \n",
    "        curr_input_arrays = {f\"c{c_idx}\": [] for c_idx in range(1, 5)}\n",
    "\n",
    "        all_found = True\n",
    "        \n",
    "        for c_idx in range(1, 5):\n",
    "            input_frame_list = raw_input[seq_key][f\"c{c_idx}\"][str(f_idx)]\n",
    "            if [None, None] in input_frame_list:\n",
    "                all_found = False\n",
    "                break\n",
    "                \n",
    "            for point_idx, joint_name in selected_names.items(): \n",
    "                pixel_coords = input_frame_list[int(point_idx)]\n",
    "                curr_input_arrays[f\"c{c_idx}\"].append(pixel_coords)\n",
    "\n",
    "                # curr_input_arrays[f\"c{c_idx}\"].append([pixel_coords[0]/img_width, pixel_coords[1]/img_height])\n",
    "                # conversion from pixels to propotions if needed\n",
    "\n",
    "        # print(curr_input_arrays)\n",
    "\n",
    "        if all_found:\n",
    "            for c_idx in range(1, 5):\n",
    "                input_frames_data[f\"c{c_idx}\"].append(np.array(curr_input_arrays[f\"c{c_idx}\"]))\n",
    "            #     print(np.array(curr_input_arrays[f\"c{c_idx}\"]).shape)\n",
    "\n",
    "            # print(curr_output_array_np.shape)    \n",
    "            output_frames_data.append(curr_output_array_np)\n",
    "        else:\n",
    "            not_found += 1\n",
    "\n",
    "print(f\"Frames with all found mocaps: {len(output_frames_data)}\")\n",
    "print(f\"Frames with at least one not found mocap: {not_found}\")\n",
    "print(f\"Proportion: {100*len(output_frames_data)/(len(output_frames_data) + not_found):.2f}%\")\n",
    "# print(input_frames_data['c4'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "344e94e4-bfe0-4f41-8b96-313a4b4c4911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MoCapInputDataset(Dataset):\n",
    "    def __init__(self, seq_keys_list, sequences, selected_names, raw_input, raw_output):\n",
    "        self.img_width = 960\n",
    "        self.img_height = 540\n",
    "        self.input_frames_data = {f\"c{c_idx}\": [] for c_idx in range(1, 5)}\n",
    "        self.output_frames_data = []\n",
    "        self.not_found = 0\n",
    "              \n",
    "        for seq_key in seq_keys_list:\n",
    "            for f_idx in range(sequences[seq_key]['number_of_frames']):\n",
    "                curr_output_array = []\n",
    "                output_frame_dict = raw_output[seq_key][f_idx]\n",
    "                for point_idx, joint_name in selected_names.items():\n",
    "                    curr_output_array.append(output_frame_dict[joint_name])\n",
    "        \n",
    "                curr_output_array_np = np.array(curr_output_array)*255\n",
    "                # 255 multiplier added to mocap to obtain distance in mm\n",
    "                curr_input_arrays = {f\"c{c_idx}\": [] for c_idx in range(1, 5)}\n",
    "        \n",
    "                all_found = True\n",
    "                \n",
    "                for c_idx in range(1, 5):\n",
    "                    input_frame_list = raw_input[seq_key][f\"c{c_idx}\"][str(f_idx)]\n",
    "                    if [None, None] in input_frame_list:\n",
    "                        all_found = False\n",
    "                        break\n",
    "                        \n",
    "                    for point_idx, joint_name in selected_names.items(): \n",
    "                        pixel_coords = input_frame_list[int(point_idx)]\n",
    "                        curr_input_arrays[f\"c{c_idx}\"].append(pixel_coords)\n",
    "                        # curr_input_arrays[f\"c{c_idx}\"].append(\n",
    "                        #     [pixel_coords[0]/self.img_width, \n",
    "                        #      pixel_coords[1]/self.img_height])\n",
    "        \n",
    "                if all_found:\n",
    "                    for c_idx in range(1, 5):\n",
    "                        self.input_frames_data[f\"c{c_idx}\"].append(np.array(curr_input_arrays[f\"c{c_idx}\"]))\n",
    " \n",
    "                    self.output_frames_data.append(curr_output_array_np)\n",
    "                else:\n",
    "                    self.not_found += 1\n",
    "\n",
    "        self.length = len(self.output_frames_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = [torch.from_numpy(self.input_frames_data[f\"c{c_idx}\"][idx]).float() for c_idx in range(1, 5)]  # each: (12, 2)\n",
    "        target = torch.from_numpy(self.output_frames_data[idx]).float()  # (12, 3)\n",
    "        return inputs, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "892c20b1-9c87-462e-9fbd-525c9d6164be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = MoCapInputDataset(train_seq_set, sequences, selected_names, raw_input, raw_output)\n",
    "valid_ds = MoCapInputDataset(valid_seq_set, sequences, selected_names, raw_input, raw_output)\n",
    "test_ds = MoCapInputDataset(test_seq_set, sequences, selected_names, raw_input, raw_output)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(valid_ds, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d1f07f7a-c1ad-445d-a9e3-7a508024efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNet, self).__init__()\n",
    "        # shape (12, 2) -> reshape to (2, 12) \n",
    "        self.conv1d = nn.Conv1d(in_channels=2, out_channels=1, kernel_size=3)  # (2, 12) -> (1, 10)\n",
    "        # flatten 4 x 10 -> 40 x 1\n",
    "        self.fc1 = nn.Linear(40, 64)\n",
    "        self.dropout1 = nn.Dropout(p=0.3)\n",
    "        self.fc2 = nn.Linear(64, 72)\n",
    "        self.dropout2 = nn.Dropout(p=0.3)\n",
    "        self.fc3 = nn.Linear(72, 36)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: 4 tensors of shape (batch, 12, 2)\n",
    "        conv_outs = []\n",
    "        for xi in x:\n",
    "            xi = xi.permute(0, 2, 1)  # reshape to (batch, 2, 12) \n",
    "            conv = self.conv1d(xi)     # (batch, 1, 10)\n",
    "            conv = conv.squeeze(1)     # (batch, 10)\n",
    "            conv_outs.append(conv)\n",
    "\n",
    "        concat = torch.cat(conv_outs, dim=1)  # (batch, 40)\n",
    "\n",
    "        out = F.relu(self.fc1(concat))\n",
    "        out = self.dropout1(out)\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.dropout2(out)\n",
    "        out = self.fc3(out)  # (batch, 36)\n",
    "        out = out.view(-1, 12, 3)  # reshape to (batch, 12, 3)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2cc4302-a879-4d2d-9612-a62724207e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MPJPE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        # shape (batch, 12, 3)\n",
    "        # compute euclidean distance for each point pair\n",
    "        distances = torch.norm(predictions - targets, dim=2)\n",
    "        mean_distance = distances.mean()\n",
    "        return mean_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35ca85b4-fb3e-488a-8ab9-ea1e41d9b15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train MPJPE = 1580.5001, Val MPJPE = 1281.9271\n",
      "Epoch 2: Train MPJPE = 1331.4666, Val MPJPE = 1260.6357\n",
      "Epoch 3: Train MPJPE = 1327.2034, Val MPJPE = 1254.5521\n",
      "Epoch 4: Train MPJPE = 1320.0655, Val MPJPE = 1244.5883\n",
      "Epoch 5: Train MPJPE = 1301.3921, Val MPJPE = 1210.2246\n",
      "Epoch 6: Train MPJPE = 1206.0619, Val MPJPE = 1034.7743\n",
      "Epoch 7: Train MPJPE = 1001.4151, Val MPJPE = 814.1994\n",
      "Epoch 8: Train MPJPE = 754.1588, Val MPJPE = 454.2428\n",
      "Epoch 9: Train MPJPE = 605.9131, Val MPJPE = 373.6592\n",
      "Epoch 10: Train MPJPE = 556.0895, Val MPJPE = 359.2036\n",
      "Epoch 11: Train MPJPE = 536.9649, Val MPJPE = 344.6037\n",
      "Epoch 12: Train MPJPE = 522.0633, Val MPJPE = 363.2411\n",
      "Epoch 13: Train MPJPE = 511.3151, Val MPJPE = 290.5614\n",
      "Epoch 14: Train MPJPE = 505.8306, Val MPJPE = 311.0294\n",
      "Epoch 15: Train MPJPE = 499.4427, Val MPJPE = 310.6782\n",
      "Epoch 16: Train MPJPE = 491.0160, Val MPJPE = 300.7964\n",
      "Epoch 17: Train MPJPE = 483.4201, Val MPJPE = 308.7988\n",
      "Epoch 18: Train MPJPE = 474.3529, Val MPJPE = 302.2669\n",
      "Epoch 19: Train MPJPE = 481.6593, Val MPJPE = 297.6869\n",
      "Epoch 20: Train MPJPE = 466.6388, Val MPJPE = 287.3124\n",
      "Epoch 21: Train MPJPE = 466.2986, Val MPJPE = 285.4729\n",
      "Epoch 22: Train MPJPE = 463.6640, Val MPJPE = 278.4939\n",
      "Epoch 23: Train MPJPE = 457.9633, Val MPJPE = 281.6292\n",
      "Epoch 24: Train MPJPE = 469.3444, Val MPJPE = 297.8341\n",
      "Epoch 25: Train MPJPE = 456.7534, Val MPJPE = 272.8914\n",
      "Epoch 26: Train MPJPE = 450.7886, Val MPJPE = 283.0930\n",
      "Epoch 27: Train MPJPE = 450.3568, Val MPJPE = 280.4510\n",
      "Epoch 28: Train MPJPE = 453.7297, Val MPJPE = 273.9108\n",
      "Epoch 29: Train MPJPE = 446.5482, Val MPJPE = 273.6594\n",
      "Epoch 30: Train MPJPE = 442.9190, Val MPJPE = 274.7205\n",
      "Epoch 31: Train MPJPE = 444.4075, Val MPJPE = 259.9740\n",
      "Epoch 32: Train MPJPE = 445.4978, Val MPJPE = 250.7201\n",
      "Epoch 33: Train MPJPE = 442.7823, Val MPJPE = 261.1042\n",
      "Epoch 34: Train MPJPE = 440.4110, Val MPJPE = 263.6032\n",
      "Epoch 35: Train MPJPE = 437.2447, Val MPJPE = 255.9864\n",
      "Epoch 36: Train MPJPE = 434.7309, Val MPJPE = 261.5321\n",
      "Epoch 37: Train MPJPE = 437.2169, Val MPJPE = 258.1384\n",
      "Epoch 38: Train MPJPE = 436.5136, Val MPJPE = 260.5819\n",
      "Epoch 39: Train MPJPE = 443.4662, Val MPJPE = 261.6329\n",
      "Epoch 40: Train MPJPE = 439.5141, Val MPJPE = 265.2671\n",
      "Epoch 41: Train MPJPE = 441.5215, Val MPJPE = 267.1585\n",
      "Epoch 42: Train MPJPE = 442.4514, Val MPJPE = 257.2362\n",
      "Epoch 43: Train MPJPE = 433.4110, Val MPJPE = 256.5461\n",
      "Epoch 44: Train MPJPE = 433.4268, Val MPJPE = 268.8772\n",
      "Epoch 45: Train MPJPE = 434.9331, Val MPJPE = 260.5153\n",
      "Epoch 46: Train MPJPE = 432.7741, Val MPJPE = 290.9148\n",
      "Epoch 47: Train MPJPE = 442.7761, Val MPJPE = 263.0825\n",
      "Epoch 48: Train MPJPE = 432.8836, Val MPJPE = 271.8135\n",
      "Epoch 49: Train MPJPE = 432.8196, Val MPJPE = 249.4450\n",
      "Epoch 50: Train MPJPE = 435.2464, Val MPJPE = 249.3726\n"
     ]
    }
   ],
   "source": [
    "model = CustomNet()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "# criterion = torch.nn.MSELoss()\n",
    "criterion = MPJPE()\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs = [inp.float() for inp in inputs]\n",
    "        targets = targets.float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * targets.size(0)\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = [inp.float() for inp in inputs]\n",
    "            targets = targets.float()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item() * targets.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train MPJPE = {avg_train_loss:.4f}, Val MPJPE = {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d14e160c-abb0-499a-9704-1bf0d5b2fc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss = 234.9538\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item() * targets.size(0)\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print(f\"Test loss = {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc4859f1-23f8-4258-88f1-52e73fe1f6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(model, inputs_list, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    inputs = [torch.from_numpy(inp).float().unsqueeze(0).to(device) for inp in inputs_list]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(inputs) \n",
    "    \n",
    "    return output.squeeze(0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05504998-252d-43f4-a2de-dc1094ee3dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8.313590049743652, 3.798708915710449, 0.7660826444625854],\n",
       " [8.503310203552246, 1.9807288646697998, 0.5631764531135559],\n",
       " [8.67428207397461, 0.3377190828323364, 0.38371729850769043],\n",
       " [8.385348320007324, 3.7756404876708984, -0.361005961894989],\n",
       " [8.568338394165039, 1.9954566955566406, -0.2797110378742218],\n",
       " [8.740983963012695, 0.31866610050201416, -0.20210732519626617],\n",
       " [8.44154167175293, 5.744988918304443, 0.8683866262435913],\n",
       " [8.519126892089844, 4.909095287322998, 1.8944714069366455],\n",
       " [8.377540588378906, 4.756703853607178, 2.5212531089782715],\n",
       " [8.439878463745117, 5.685400485992432, -0.36643746495246887],\n",
       " [8.516109466552734, 4.839725494384766, -1.3964648246765137],\n",
       " [8.319049835205078, 4.764559745788574, -2.0099925994873047]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(raw_output[test_seq_set[0]][0].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "688109ed-9ebc-45a3-8ae6-28968d7d4588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p31s7\n",
      "[[6.9573774337768555, 3.990058183670044, -0.9458487033843994], [6.790740966796875, 2.108768939971924, -0.6557729244232178], [6.641436576843262, 0.4229142665863037, -0.39575472474098206], [6.779537200927734, 3.968749761581421, 0.7932784557342529], [7.422176361083984, 2.181001901626587, 0.5760002732276917], [6.857697010040283, 0.5481630563735962, 0.29319626092910767], [6.661077976226807, 5.7644195556640625, -1.014063835144043], [6.555835247039795, 4.422194957733154, -1.3695333003997803], [7.137636184692383, 4.12354850769043, -1.546125888824463], [6.542348384857178, 5.848693370819092, 0.5578198432922363], [5.999451637268066, 4.781178951263428, 0.8451588153839111], [6.1203436851501465, 4.0845947265625, 1.1242176294326782]]\n",
      "\n",
      "[[-118.12306939199553, 1673.1732366172698, 123.56820019962382], [76.37843527666733, 1675.2784158202187, 185.91329433397286], [-120.98718373185523, 1689.6438855787865, 539.2161825129382], [112.75039950315202, 1737.7942719420732, 564.7950584474041], [-178.32282279329667, 1601.5238771240956, 910.9186509853288], [97.8393918367824, 1571.3924292235927, 920.4864325006043], [-277.38243625732235, 1593.9678356397246, 1415.9452032553932], [161.20725835283142, 1554.7076781179953, 1428.9123503820585], [-370.6803841551924, 1643.7004928555573, 1150.9377646222076], [248.45771363780094, 1521.475746376262, 1183.157329923052], [-422.7162583754253, 1768.8337852021368, 957.5052111960216], [308.9402099791819, 1572.7137738719257, 967.8128105450797]]\n"
     ]
    }
   ],
   "source": [
    "test_seq = test_seq_set[-1]\n",
    "print(test_seq)\n",
    "frame = 100\n",
    "\n",
    "bvh_sample_data = list(raw_output[test_seq][frame].values())\n",
    "triangulation_sample_all_data = triangulation_data[test_seq][frame]\n",
    "triangulation_sample_data = [triangulation_sample_all_data[int(j_idx)] for j_idx in selected_names.keys()]\n",
    "print(bvh_sample_data)\n",
    "print()\n",
    "print(triangulation_sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73cf4ecc-a65d-4e20-99f3-e508e82cb239",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.76316017, 0.68825501],\n",
       "        [0.77828461, 0.69167244],\n",
       "        [0.7688598 , 0.57809174],\n",
       "        [0.80211681, 0.58096778],\n",
       "        [0.74845439, 0.48069626],\n",
       "        [0.77233142, 0.47910824],\n",
       "        [0.7542181 , 0.32151055],\n",
       "        [0.79155314, 0.33582935],\n",
       "        [0.75089961, 0.41073531],\n",
       "        [0.783611  , 0.40402964],\n",
       "        [0.76022077, 0.48667321],\n",
       "        [0.79006279, 0.47789776]]),\n",
       " array([[0.5083378 , 0.69301528],\n",
       "        [0.47600701, 0.67435449],\n",
       "        [0.50878209, 0.57964247],\n",
       "        [0.46967804, 0.58347642],\n",
       "        [0.51285273, 0.46745655],\n",
       "        [0.46622449, 0.45879248],\n",
       "        [0.53639412, 0.30874494],\n",
       "        [0.46251309, 0.29740098],\n",
       "        [0.55061233, 0.38244581],\n",
       "        [0.44634911, 0.375168  ],\n",
       "        [0.56295902, 0.43414575],\n",
       "        [0.43660882, 0.43939573]]),\n",
       " array([[0.22855243, 0.69278949],\n",
       "        [0.23841311, 0.65525663],\n",
       "        [0.21662265, 0.57509148],\n",
       "        [0.23195376, 0.55330348],\n",
       "        [0.21496983, 0.46950275],\n",
       "        [0.24622121, 0.46947035],\n",
       "        [0.20311236, 0.32669571],\n",
       "        [0.25128967, 0.31703606],\n",
       "        [0.1893954 , 0.41004041],\n",
       "        [0.26075986, 0.38648301],\n",
       "        [0.16245456, 0.46292821],\n",
       "        [0.25495768, 0.44472396]]),\n",
       " array([[0.49777153, 0.51184291],\n",
       "        [0.51420122, 0.50282198],\n",
       "        [0.497697  , 0.44741499],\n",
       "        [0.51513851, 0.44302857],\n",
       "        [0.49281958, 0.38978732],\n",
       "        [0.51455182, 0.38742161],\n",
       "        [0.48538792, 0.31425229],\n",
       "        [0.52046216, 0.31037512],\n",
       "        [0.47690016, 0.35297692],\n",
       "        [0.52878368, 0.35150686],\n",
       "        [0.47376749, 0.3813231 ],\n",
       "        [0.536134  , 0.38541996]])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_width = 960\n",
    "img_height = 540\n",
    "\n",
    "mp_input_sample = []\n",
    "\n",
    "for c_idx in range(1, 5):\n",
    "    all_frames_for_camera = raw_input[test_seq][f\"c{c_idx}\"][str(frame)]\n",
    "    camera_mp_input_sample = []\n",
    "    \n",
    "    for point_idx, joint_name in selected_names.items(): \n",
    "        pixel_coords = all_frames_for_camera[int(point_idx)]\n",
    "        camera_mp_input_sample.append(pixel_coords)\n",
    "        # camera_mp_input_sample.append([pixel_coords[0]/img_width, pixel_coords[1]/img_height])\n",
    "\n",
    "    mp_input_sample.append(np.array(camera_mp_input_sample))\n",
    "    \n",
    "mp_input_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a4b9737-6623-4a1e-be25-d3dd476309e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1637.281   ,  104.79422 ,  -68.8728  ],\n",
       "       [1645.0094  ,  105.59387 ,   20.323442],\n",
       "       [1700.0118  ,  465.3472  , -103.194374],\n",
       "       [1705.5361  ,  460.60898 ,   46.954372],\n",
       "       [1654.883   ,  864.1387  , -134.34045 ],\n",
       "       [1651.0089  ,  861.4031  ,   79.19628 ],\n",
       "       [1634.5836  , 1304.4253  , -173.01857 ],\n",
       "       [1625.2422  , 1304.3583  ,  100.3998  ],\n",
       "       [1621.0388  , 1023.0364  , -205.30948 ],\n",
       "       [1596.331   , 1020.94617 ,  125.41394 ],\n",
       "       [1680.657   ,  886.38727 , -216.04456 ],\n",
       "       [1646.4257  ,  877.7934  ,  145.09079 ]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = predict_single(model, mp_input_sample, 'cpu')\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d96e453-d8b5-4321-8409-a2d47635d0e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"820px\"\n",
       "    height=\"820\"\n",
       "    src=\"iframe_figures/figure_38.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe'\n",
    "\n",
    "SCALE_FACTOR = 255\n",
    "\n",
    "x = [vec[2]*SCALE_FACTOR for vec in bvh_sample_data]\n",
    "y = [vec[0]*SCALE_FACTOR for vec in bvh_sample_data]\n",
    "z = [vec[1]*SCALE_FACTOR for vec in bvh_sample_data]\n",
    "\n",
    "# x_t = [vec[0]/SCALE_FACTOR for vec in triangulation_sample_data]\n",
    "# y_t = [vec[1]/SCALE_FACTOR for vec in triangulation_sample_data]\n",
    "# z_t = [vec[2]/SCALE_FACTOR for vec in triangulation_sample_data]\n",
    "\n",
    "x_t = [vec[0] for vec in triangulation_sample_data]\n",
    "y_t = [vec[1] for vec in triangulation_sample_data]\n",
    "z_t = [vec[2] for vec in triangulation_sample_data]\n",
    "\n",
    "x_p = [vec[2] for vec in predicted]\n",
    "y_p = [vec[0] for vec in predicted]\n",
    "z_p = [vec[1] for vec in predicted]\n",
    "    \n",
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Scatter3d(\n",
    "            x=x, y=y, z=z,\n",
    "            mode='markers',\n",
    "            marker=dict(size=5, color='blue'),\n",
    "            hoverinfo='text',\n",
    "            name='Joints BVH'),\n",
    "        go.Scatter3d(\n",
    "            x=x_t, y=y_t, z=z_t,\n",
    "            mode='markers',\n",
    "            marker=dict(size=5, color='red'),\n",
    "            hoverinfo='text',\n",
    "            name='Joints triangulation mediapipe'),\n",
    "        go.Scatter3d(\n",
    "            x=x_p, y=y_p, z=z_p,\n",
    "            mode='markers',\n",
    "            marker=dict(size=5, color='green'),\n",
    "            hoverinfo='text',\n",
    "            name='Predicted by NN'),\n",
    "        ]\n",
    ")\n",
    "\n",
    "fig.update_layout(scene=dict(\n",
    "    xaxis_title='X',\n",
    "    yaxis_title='Y',\n",
    "    zaxis_title='Z',\n",
    "    xaxis=dict(range=[-6000, 6000]),\n",
    "    yaxis=dict(range=[-6000, 6000]),\n",
    "    zaxis=dict(range=[-6000, 6000]),\n",
    "    aspectmode='cube', \n",
    "),\n",
    "title='3D joints plot from bvh file',\n",
    "width=800,\n",
    "height=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db610651-515e-437b-aaf3-b9c3e295777b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
