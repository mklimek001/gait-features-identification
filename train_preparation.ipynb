{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0191c29c-7f7e-4780-b870-fff289599da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a0cb655-603b-4038-b5f4-4ecd67f4c801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 2])\n",
      "tensor([[[ 1.,  2.],\n",
      "         [ 3.,  4.],\n",
      "         [ 5.,  6.],\n",
      "         [ 7.,  8.],\n",
      "         [ 9., 10.]]])\n",
      "torch.Size([1, 3, 1])\n",
      "tensor([[[1.4459],\n",
      "         [1.3017],\n",
      "         [0.9227]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# x = torch.randn(1, 5, 2)\n",
    "x = torch.from_numpy(np.array([[[1,2], [3,4], [5,6], [7, 8], [9, 10]]])).float()\n",
    "print(x.shape)\n",
    "print(x)\n",
    "\n",
    "conv1d = nn.Conv1d(in_channels=5, out_channels=3, kernel_size=2)\n",
    "\n",
    "output = conv1d(x)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dbbccff-0460-4240-87ca-753e5b033fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.,  2.],\n",
      "         [ 3.,  4.],\n",
      "         [ 5.,  6.],\n",
      "         [ 7.,  8.],\n",
      "         [ 9., 10.]]])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4b010f8-18d7-4c1e-bf03-09cee86baf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.4459],\n",
      "         [1.3017],\n",
      "         [0.9227]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94b66fd4-c06a-4c80-94bc-5d43d81effde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences with mocap data: 152\n",
      "Number of sequences with mocap data and parallel cameras: 76\n",
      "Number of sequences with mocap data, parallel cameras and after clothing change: 12\n",
      "Number of unique participants with mocap data and parallel cameras: 32\n",
      "Number of unique participants with mocap data, parallel cameras and after clothing change: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'p26', 'p27', 'p28', 'p29', 'p30', 'p31'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scripts.parsers import parse_sequences as parse_sequence_info\n",
    "\n",
    "file_path = 'gait3d\\\\ListOfSequences.txt'\n",
    "sequences = parse_sequence_info(file_path)\n",
    "\n",
    "mocap_keys = []\n",
    "par_cam_keys = []\n",
    "par_cam_person = set()\n",
    "par_after_cloth_change_keys = []\n",
    "par_after_cloth_change_person = set()\n",
    "\n",
    "for key, params in sequences.items():\n",
    "    if params['MoCap_data']:\n",
    "        mocap_keys.append(key)\n",
    "        if key[-1] in [\"1\", \"3\", \"5\", \"7\"]:\n",
    "            par_cam_keys.append(key)\n",
    "            par_cam_person.add(key[:-2])\n",
    "        if key[-1] in [\"5\", \"7\"]:\n",
    "            par_after_cloth_change_keys.append(key)\n",
    "            par_after_cloth_change_person.add(key[:-2])\n",
    "\n",
    "print(f\"Number of sequences with mocap data: {len(mocap_keys)}\")\n",
    "print(f\"Number of sequences with mocap data and parallel cameras: {len(par_cam_keys)}\")\n",
    "print(f\"Number of sequences with mocap data, parallel cameras and after clothing change: {len(par_after_cloth_change_keys)}\")\n",
    "print(f\"Number of unique participants with mocap data and parallel cameras: {len(par_cam_person)}\")\n",
    "print(f\"Number of unique participants with mocap data, parallel cameras and after clothing change: {len(par_after_cloth_change_person)}\")\n",
    "par_after_cloth_change_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3395950f-6678-499e-a788-0d60ec5327ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test sequences: ['p6s1', 'p6s3', 'p19s1', 'p19s3', 'p32s1', 'p32s3', 'p29s5', 'p29s7', 'p30s5', 'p30s7']\n",
      "valid sequences: ['p23s1', 'p23s3', 'p12s1', 'p12s3', 'p8s1', 'p8s3', 'p26s5', 'p26s7', 'p28s5', 'p28s7']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "without_clothing_change = []\n",
    "while len(without_clothing_change) < 6:\n",
    "    random_person = random.choice(list(par_cam_person))\n",
    "    if random_person not in par_after_cloth_change_person:\n",
    "        without_clothing_change.append(random_person)\n",
    "        par_cam_person.remove(random_person)\n",
    "\n",
    "with_clothing_change = []\n",
    "while len(with_clothing_change) < 4:\n",
    "    random_person = random.choice(list(par_after_cloth_change_person))\n",
    "    with_clothing_change.append(random_person)\n",
    "    par_after_cloth_change_person.remove(random_person)\n",
    "\n",
    "\n",
    "test_seq_set = ([f'{p_seq}s{seq_idx}' for p_seq in without_clothing_change[:3] for seq_idx in [1, 3]] +\n",
    "                [f'{p_seq}s{seq_idx}' for p_seq in with_clothing_change[:2] for seq_idx in [5, 7]])\n",
    "\n",
    "valid_seq_set = ([f'{p_seq}s{seq_idx}' for p_seq in without_clothing_change[3:] for seq_idx in [1, 3]] +\n",
    "                [f'{p_seq}s{seq_idx}' for p_seq in with_clothing_change[2:] for seq_idx in [5, 7]])\n",
    "\n",
    "print(f\"test sequences: {test_seq_set}\")\n",
    "print(f\"valid sequences: {valid_seq_set}\")\n",
    "# without_clothing_change + [only_after_clothing_change] + [only_before_clothing_change]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac4b8fb8-e46b-4102-916d-869bc04310e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sequences: ['p17s1', 'p17s3', 'p11s1', 'p11s3', 'p25s1', 'p25s3', 'p26s1', 'p26s3', 'p20s1', 'p20s3', 'p9s1', 'p9s3', 'p10s1', 'p10s3', 'p31s1', 'p31s3', 'p22s1', 'p22s3', 'p1s1', 'p1s3', 'p13s1', 'p13s3', 'p27s1', 'p27s3', 'p24s1', 'p24s3', 'p3s1', 'p3s3', 'p5s1', 'p5s3', 'p2s1', 'p2s3', 'p14s1', 'p14s3', 'p7s1', 'p7s3', 'p21s1', 'p21s3', 'p30s1', 'p30s3', 'p29s1', 'p29s3', 'p18s1', 'p18s3', 'p4s1', 'p4s3', 'p16s1', 'p16s3', 'p15s1', 'p15s3', 'p28s1', 'p28s3', 'p31s5', 'p31s7', 'p27s5', 'p27s7']\n"
     ]
    }
   ],
   "source": [
    "train_seq_set = ([f'{p_seq}s{seq_idx}' for p_seq in list(par_cam_person) for seq_idx in [1, 3]] +\n",
    "                 [f'{p_seq}s{seq_idx}' for p_seq in list(par_after_cloth_change_person) for seq_idx in [5, 7]])\n",
    "\n",
    "print(f\"train sequences: {train_seq_set}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fd1ea23-7065-456b-a8b6-288272c37503",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1s1 | train |       |      |\n",
      "p1s3 | train |       |      |\n",
      "p2s1 | train |       |      |\n",
      "p2s3 | train |       |      |\n",
      "p3s1 | train |       |      |\n",
      "p3s3 | train |       |      |\n",
      "p4s1 | train |       |      |\n",
      "p4s3 | train |       |      |\n",
      "p5s1 | train |       |      |\n",
      "p5s3 | train |       |      |\n",
      "p6s1 |       |       | test |\n",
      "p6s3 |       |       | test |\n",
      "p7s1 | train |       |      |\n",
      "p7s3 | train |       |      |\n",
      "p8s1 |       | valid |      |\n",
      "p8s3 |       | valid |      |\n",
      "p9s1 | train |       |      |\n",
      "p9s3 | train |       |      |\n",
      "p10s1 | train |       |      |\n",
      "p10s3 | train |       |      |\n",
      "p11s1 | train |       |      |\n",
      "p11s3 | train |       |      |\n",
      "p12s1 |       | valid |      |\n",
      "p12s3 |       | valid |      |\n",
      "p13s1 | train |       |      |\n",
      "p13s3 | train |       |      |\n",
      "p14s1 | train |       |      |\n",
      "p14s3 | train |       |      |\n",
      "p15s1 | train |       |      |\n",
      "p15s3 | train |       |      |\n",
      "p16s1 | train |       |      |\n",
      "p16s3 | train |       |      |\n",
      "p17s1 | train |       |      |\n",
      "p17s3 | train |       |      |\n",
      "p18s1 | train |       |      |\n",
      "p18s3 | train |       |      |\n",
      "p19s1 |       |       | test |\n",
      "p19s3 |       |       | test |\n",
      "p20s1 | train |       |      |\n",
      "p20s3 | train |       |      |\n",
      "p21s1 | train |       |      |\n",
      "p21s3 | train |       |      |\n",
      "p22s1 | train |       |      |\n",
      "p22s3 | train |       |      |\n",
      "p23s1 |       | valid |      |\n",
      "p23s3 |       | valid |      |\n",
      "p24s1 | train |       |      |\n",
      "p24s3 | train |       |      |\n",
      "p25s1 | train |       |      |\n",
      "p25s3 | train |       |      |\n",
      "p26s1 | train |       |      |\n",
      "p26s3 | train |       |      |\n",
      "p26s5 |       | valid |      |\n",
      "p26s7 |       | valid |      |\n",
      "p27s1 | train |       |      |\n",
      "p27s3 | train |       |      |\n",
      "p27s5 | train |       |      |\n",
      "p27s7 | train |       |      |\n",
      "p28s1 | train |       |      |\n",
      "p28s3 | train |       |      |\n",
      "p28s5 |       | valid |      |\n",
      "p28s7 |       | valid |      |\n",
      "p29s1 | train |       |      |\n",
      "p29s3 | train |       |      |\n",
      "p29s5 |       |       | test |\n",
      "p29s7 |       |       | test |\n",
      "p30s1 | train |       |      |\n",
      "p30s3 | train |       |      |\n",
      "p30s5 |       |       | test |\n",
      "p30s7 |       |       | test |\n",
      "p31s1 | train |       |      |\n",
      "p31s3 | train |       |      |\n",
      "p31s5 | train |       |      |\n",
      "p31s7 | train |       |      |\n",
      "p32s1 |       |       | test |\n",
      "p32s3 |       |       | test |\n"
     ]
    }
   ],
   "source": [
    "for key, params in sequences.items():\n",
    "    if params['MoCap_data']:\n",
    "        if key[-1] in [\"1\", \"3\", \"5\", \"7\"]:\n",
    "            print(f\"{key} | {'train' if key in train_seq_set else '     '} | {'valid' if key in valid_seq_set else '     '} | {'test' if key in test_seq_set else '    '} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af60bc52-4453-4753-baf2-8650f1b9b70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 56 | 77.78%\n",
      "Test size: 10 | 13.89%\n",
      "Valid size: 10 | 13.89%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train size: {len(train_seq_set)} | {100*len(train_seq_set)/72:.2f}%\")\n",
    "print(f\"Test size: {len(test_seq_set)} | {100*len(test_seq_set)/72:.2f}%\")\n",
    "print(f\"Valid size: {len(valid_seq_set)} | {100*len(valid_seq_set)/72:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a0cbbf8-68b8-4fd5-a614-a293ec8854a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'27': 'lfoot',\n",
       " '28': 'rfoot',\n",
       " '25': 'ltibia',\n",
       " '26': 'rtibia',\n",
       " '23': 'lfemur',\n",
       " '24': 'rfemur',\n",
       " '11': 'lhumerus',\n",
       " '12': 'rhumerus'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "selected_names_file = \"./datasets/mediapipe/selected_joint_names.json\"\n",
    "input_data_file = \"./datasets/mediapipe/dataset_v2.json\"\n",
    "output_data_file = \"./datasets/mocap/dataset_v2.json\"\n",
    "\n",
    "with open(input_data_file, 'r') as file:\n",
    "    raw_input = json.load(file)\n",
    "\n",
    "with open(output_data_file, 'r') as file:\n",
    "    raw_output = json.load(file)\n",
    "\n",
    "with open(selected_names_file, 'r') as file:\n",
    "    selected_names = json.load(file)\n",
    "\n",
    "selected_names.pop('15')\n",
    "selected_names.pop('16')\n",
    "selected_names.pop('13')\n",
    "selected_names.pop('14')\n",
    "selected_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8138529-b136-4a38-aeb8-2fd670c8b52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "triang_data_file = \"./datasets/mediapipe/triangulation.json\"\n",
    "\n",
    "with open(triang_data_file, 'r') as file:\n",
    "    triangulation_data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac3d5644-056f-49ee-8f7a-7576605b1d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start_frame': 195,\n",
       " 'number_of_frames': 135,\n",
       " 'frame_offset': 0,\n",
       " 'MoCap_data': True}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences['p1s1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "575c6d0f-1bf2-4cfd-bb23-06698b0aa8be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames with all found mocaps: 6035\n",
      "Frames with at least one not found mocap: 3495\n",
      "Proportion: 63.33%\n"
     ]
    }
   ],
   "source": [
    "input_frames_data = {f\"c{c_idx}\": [] for c_idx in range(1, 5)}\n",
    "output_frames_data = []\n",
    "img_width = 960\n",
    "img_height = 540\n",
    "\n",
    "not_found = 0\n",
    "seq_keys_list = train_seq_set + test_seq_set + valid_seq_set\n",
    "\n",
    "for seq_key in seq_keys_list:\n",
    "    for f_idx in range(sequences[seq_key]['number_of_frames']):\n",
    "    # for f_idx in range(2):\n",
    "        curr_output_array = []\n",
    "        output_frame_dict = raw_output[seq_key][f_idx]\n",
    "        for point_idx, joint_name in selected_names.items():\n",
    "            curr_output_array.append(output_frame_dict[joint_name])\n",
    "\n",
    "        curr_output_array_np = np.array(curr_output_array)\n",
    "        # print(curr_output_array_np)\n",
    "        \n",
    "        curr_input_arrays = {f\"c{c_idx}\": [] for c_idx in range(1, 5)}\n",
    "\n",
    "        all_found = True\n",
    "        \n",
    "        for c_idx in range(1, 5):\n",
    "            input_frame_list = raw_input[seq_key][f\"c{c_idx}\"][str(f_idx)]\n",
    "            if [None, None] in input_frame_list:\n",
    "                all_found = False\n",
    "                break\n",
    "                \n",
    "            for point_idx, joint_name in selected_names.items(): \n",
    "                pixel_coords = input_frame_list[int(point_idx)]\n",
    "                curr_input_arrays[f\"c{c_idx}\"].append(pixel_coords)\n",
    "\n",
    "                # curr_input_arrays[f\"c{c_idx}\"].append([pixel_coords[0]/img_width, pixel_coords[1]/img_height])\n",
    "                # conversion from pixels to propotions if needed\n",
    "\n",
    "        # print(curr_input_arrays)\n",
    "\n",
    "        if all_found:\n",
    "            for c_idx in range(1, 5):\n",
    "                input_frames_data[f\"c{c_idx}\"].append(np.array(curr_input_arrays[f\"c{c_idx}\"]))\n",
    "            #     print(np.array(curr_input_arrays[f\"c{c_idx}\"]).shape)\n",
    "\n",
    "            # print(curr_output_array_np.shape)    \n",
    "            output_frames_data.append(curr_output_array_np)\n",
    "        else:\n",
    "            not_found += 1\n",
    "\n",
    "print(f\"Frames with all found mocaps: {len(output_frames_data)}\")\n",
    "print(f\"Frames with at least one not found mocap: {not_found}\")\n",
    "print(f\"Proportion: {100*len(output_frames_data)/(len(output_frames_data) + not_found):.2f}%\")\n",
    "# print(input_frames_data['c4'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "344e94e4-bfe0-4f41-8b96-313a4b4c4911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MoCapInputDataset(Dataset):\n",
    "    def __init__(self, seq_keys_list, sequences, selected_names, raw_input, raw_output):\n",
    "        self.img_width = 960\n",
    "        self.img_height = 540\n",
    "        self.input_frames_data = {f\"c{c_idx}\": [] for c_idx in range(1, 5)}\n",
    "        self.output_frames_data = []\n",
    "        self.not_found = 0\n",
    "              \n",
    "        for seq_key in seq_keys_list:\n",
    "            for f_idx in range(sequences[seq_key]['number_of_frames']):\n",
    "                curr_output_array = []\n",
    "                output_frame_dict = raw_output[seq_key][f_idx]\n",
    "                for point_idx, joint_name in selected_names.items():\n",
    "                    curr_output_array.append(output_frame_dict[joint_name])\n",
    "        \n",
    "                curr_output_array_np = np.array(curr_output_array)*255\n",
    "                # 255 multiplier added to mocap to obtain distance in mm\n",
    "                curr_input_arrays = {f\"c{c_idx}\": [] for c_idx in range(1, 5)}\n",
    "        \n",
    "                all_found = True\n",
    "                \n",
    "                for c_idx in range(1, 5):\n",
    "                    input_frame_list = raw_input[seq_key][f\"c{c_idx}\"][str(f_idx)]\n",
    "                    if [None, None] in input_frame_list:\n",
    "                        all_found = False\n",
    "                        break\n",
    "                        \n",
    "                    for point_idx, joint_name in selected_names.items(): \n",
    "                        pixel_coords = input_frame_list[int(point_idx)]\n",
    "                        curr_input_arrays[f\"c{c_idx}\"].append(pixel_coords)\n",
    "                        # curr_input_arrays[f\"c{c_idx}\"].append(\n",
    "                        #     [pixel_coords[0]/self.img_width, \n",
    "                        #      pixel_coords[1]/self.img_height])\n",
    "        \n",
    "                if all_found:\n",
    "                    for c_idx in range(1, 5):\n",
    "                        self.input_frames_data[f\"c{c_idx}\"].append(np.array(curr_input_arrays[f\"c{c_idx}\"]))\n",
    " \n",
    "                    self.output_frames_data.append(curr_output_array_np)\n",
    "                else:\n",
    "                    self.not_found += 1\n",
    "\n",
    "        self.length = len(self.output_frames_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = [torch.from_numpy(self.input_frames_data[f\"c{c_idx}\"][idx]).float() for c_idx in range(1, 5)]  # each: (12, 2)\n",
    "        target = torch.from_numpy(self.output_frames_data[idx]).float()  # (12, 3)\n",
    "        return inputs, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f156bde-e3d7-45cc-a079-92bf96a11882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna results\n",
    "best_params = {'lr': 0.005589010994074508, 'weight_decay': 1.1906353862455155e-05, 'dropout': 0.2214033785244307, 'batch_size': 64, 'activation': 'gelu'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "892c20b1-9c87-462e-9fbd-525c9d6164be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = best_params['batch_size']\n",
    "\n",
    "train_ds = MoCapInputDataset(train_seq_set, sequences, selected_names, raw_input, raw_output)\n",
    "valid_ds = MoCapInputDataset(valid_seq_set, sequences, selected_names, raw_input, raw_output)\n",
    "test_ds = MoCapInputDataset(test_seq_set, sequences, selected_names, raw_input, raw_output)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(valid_ds, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76e59766-3069-480b-a536-b60b8d19493f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([[0.5763, 0.7128],\n",
       "          [0.6832, 0.6859],\n",
       "          [0.5925, 0.6036],\n",
       "          [0.6394, 0.5961],\n",
       "          [0.6204, 0.4968],\n",
       "          [0.6190, 0.4932],\n",
       "          [0.6346, 0.3300],\n",
       "          [0.6179, 0.3310]]),\n",
       "  tensor([[0.4995, 0.6134],\n",
       "          [0.5165, 0.6481],\n",
       "          [0.4981, 0.5399],\n",
       "          [0.5199, 0.5501],\n",
       "          [0.4977, 0.4576],\n",
       "          [0.5212, 0.4579],\n",
       "          [0.4863, 0.3278],\n",
       "          [0.5287, 0.3244]]),\n",
       "  tensor([[0.4229, 0.6689],\n",
       "          [0.3215, 0.6541],\n",
       "          [0.4081, 0.5586],\n",
       "          [0.3760, 0.5602],\n",
       "          [0.3785, 0.4515],\n",
       "          [0.3695, 0.4531],\n",
       "          [0.3767, 0.2948],\n",
       "          [0.3661, 0.2951]]),\n",
       "  tensor([[0.5145, 0.4479],\n",
       "          [0.5037, 0.4243],\n",
       "          [0.5159, 0.3783],\n",
       "          [0.5023, 0.3725],\n",
       "          [0.5195, 0.3039],\n",
       "          [0.5010, 0.3042],\n",
       "          [0.5302, 0.2048],\n",
       "          [0.4948, 0.2030]])],\n",
       " tensor([[ 460.8066,   78.6099,  132.6600],\n",
       "         [1144.6643,  202.4733,   26.7038],\n",
       "         [ 467.3495,  502.0592,  167.4257],\n",
       "         [ 849.1973,  451.1764,  -12.8243],\n",
       "         [ 692.0551,  951.0314,  205.8584],\n",
       "         [ 717.5051,  942.0501,  -73.7578],\n",
       "         [ 692.8350, 1458.6259,  288.8861],\n",
       "         [ 653.4987, 1488.9353,  -67.3279]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d1f07f7a-c1ad-445d-a9e3-7a508024efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNet, self).__init__()\n",
    "        dropout = 0\n",
    "        # shape (8, 2) -> reshape to (2, 8) \n",
    "        self.conv1d1 = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=2, out_channels=8, kernel_size=2, padding=1) for _ in range(4)\n",
    "        ])\n",
    "        self.conv1d2 = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=8, out_channels=1, kernel_size=2) for _ in range(4)\n",
    "        ])\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.fc1 = nn.Linear(32, 48)\n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "        self.bn2 = nn.BatchNorm1d(48)\n",
    "        self.fc2 = nn.Linear(48, 32)\n",
    "        self.dropout2 = nn.Dropout(p=dropout)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.fc3 = nn.Linear(32, 24)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: 4 tensors of shape (batch, 8, 2)\n",
    "        conv_outs = []\n",
    "        for i, xi in enumerate(x):\n",
    "            xi = xi.permute(0, 2, 1)  # reshape to (batch, 2, 8) \n",
    "            # conv = self.conv1d[i](xi)     # (batch, 1, 8)\n",
    "            conv = self.conv1d1[i](xi)\n",
    "            conv = self.conv1d2[i](conv)\n",
    "            conv = conv.squeeze(1)     # (batch, 8)\n",
    "            conv_outs.append(conv)\n",
    "\n",
    "        concat = torch.cat(conv_outs, dim=1)  # (batch, 28)\n",
    "\n",
    "        out = self.bn1(concat)\n",
    "        out = F.gelu(self.bn2(self.fc1(out)))\n",
    "        out = self.dropout1(out)\n",
    "        out = F.gelu(self.bn3(self.fc2(out)))\n",
    "        out = self.dropout2(out)\n",
    "        out = self.fc3(out)  # (batch, 24)\n",
    "        out = out.view(-1, 8, 3)  # reshape to (batch, 8, 3)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d2cc4302-a879-4d2d-9612-a62724207e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MPJPE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        # shape (batch, 8, 3)\n",
    "        # compute euclidean distance for each point pair\n",
    "        distances = torch.norm(predictions - targets, dim=2)\n",
    "        mean_distance = distances.mean()\n",
    "        return mean_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "35ca85b4-fb3e-488a-8ab9-ea1e41d9b15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train MPJPE = 1618.7815, Val MPJPE = 1616.9589\n",
      "Epoch 2: Train MPJPE = 1586.5022, Val MPJPE = 1571.3214\n",
      "Epoch 3: Train MPJPE = 1522.8847, Val MPJPE = 1482.3061\n",
      "Epoch 4: Train MPJPE = 1430.8790, Val MPJPE = 1352.7088\n",
      "Epoch 5: Train MPJPE = 1314.2452, Val MPJPE = 1191.2985\n",
      "Epoch 6: Train MPJPE = 1177.9182, Val MPJPE = 1048.2659\n",
      "Epoch 7: Train MPJPE = 1030.5642, Val MPJPE = 885.8311\n",
      "Epoch 8: Train MPJPE = 878.0416, Val MPJPE = 746.0581\n",
      "Epoch 9: Train MPJPE = 742.7503, Val MPJPE = 671.9715\n",
      "Epoch 10: Train MPJPE = 633.2009, Val MPJPE = 624.3766\n",
      "Epoch 11: Train MPJPE = 546.8538, Val MPJPE = 527.1913\n",
      "Epoch 12: Train MPJPE = 479.3043, Val MPJPE = 493.6472\n",
      "Epoch 13: Train MPJPE = 410.8661, Val MPJPE = 324.2700\n",
      "Epoch 14: Train MPJPE = 359.4240, Val MPJPE = 255.1743\n",
      "Epoch 15: Train MPJPE = 336.0443, Val MPJPE = 249.3298\n",
      "Epoch 16: Train MPJPE = 305.0816, Val MPJPE = 235.5810\n",
      "Epoch 17: Train MPJPE = 281.5029, Val MPJPE = 261.3451\n",
      "Epoch 18: Train MPJPE = 265.8526, Val MPJPE = 226.7297\n",
      "Epoch 19: Train MPJPE = 264.1069, Val MPJPE = 208.0846\n",
      "Epoch 20: Train MPJPE = 259.3877, Val MPJPE = 301.3127\n",
      "Epoch 21: Train MPJPE = 247.3839, Val MPJPE = 231.6268\n",
      "Epoch 22: Train MPJPE = 239.2529, Val MPJPE = 336.9546\n",
      "Epoch 23: Train MPJPE = 236.7701, Val MPJPE = 255.0665\n",
      "Epoch 24: Train MPJPE = 243.1385, Val MPJPE = 292.6736\n",
      "Epoch 25: Train MPJPE = 230.0116, Val MPJPE = 318.7468\n",
      "Epoch 26: Train MPJPE = 223.6863, Val MPJPE = 295.0382\n",
      "Epoch 27: Train MPJPE = 207.4338, Val MPJPE = 300.2840\n",
      "Epoch 28: Train MPJPE = 212.6279, Val MPJPE = 398.2162\n",
      "Epoch 29: Train MPJPE = 214.4454, Val MPJPE = 325.3643\n",
      "Epoch 30: Train MPJPE = 211.2990, Val MPJPE = 373.8857\n",
      "Epoch 31: Train MPJPE = 199.3441, Val MPJPE = 347.6632\n",
      "Epoch 32: Train MPJPE = 201.5315, Val MPJPE = 396.8748\n",
      "Epoch 33: Train MPJPE = 203.0579, Val MPJPE = 326.9930\n",
      "Epoch 34: Train MPJPE = 204.5213, Val MPJPE = 385.9659\n",
      "Epoch 35: Train MPJPE = 199.8352, Val MPJPE = 379.2051\n",
      "Epoch 36: Train MPJPE = 207.1538, Val MPJPE = 379.7629\n",
      "Epoch 37: Train MPJPE = 201.3842, Val MPJPE = 333.3112\n",
      "Epoch 38: Train MPJPE = 213.0998, Val MPJPE = 381.9063\n",
      "Epoch 39: Train MPJPE = 202.7007, Val MPJPE = 330.8903\n",
      "Epoch 40: Train MPJPE = 190.1656, Val MPJPE = 424.3511\n",
      "Epoch 41: Train MPJPE = 193.6209, Val MPJPE = 339.3284\n",
      "Epoch 42: Train MPJPE = 196.7410, Val MPJPE = 350.8120\n",
      "Epoch 43: Train MPJPE = 192.8763, Val MPJPE = 426.0226\n",
      "Epoch 44: Train MPJPE = 190.1435, Val MPJPE = 379.3204\n",
      "Epoch 45: Train MPJPE = 191.2332, Val MPJPE = 393.8144\n",
      "Epoch 46: Train MPJPE = 184.4834, Val MPJPE = 360.2592\n",
      "Epoch 47: Train MPJPE = 185.4693, Val MPJPE = 426.0029\n",
      "Epoch 48: Train MPJPE = 190.0384, Val MPJPE = 326.5454\n",
      "Epoch 49: Train MPJPE = 180.9488, Val MPJPE = 424.1599\n",
      "Epoch 50: Train MPJPE = 188.6392, Val MPJPE = 393.0005\n",
      "Epoch 51: Train MPJPE = 194.2828, Val MPJPE = 587.1744\n",
      "Epoch 52: Train MPJPE = 191.6585, Val MPJPE = 353.2627\n",
      "Epoch 53: Train MPJPE = 174.7360, Val MPJPE = 337.1648\n",
      "Epoch 54: Train MPJPE = 187.0930, Val MPJPE = 347.1734\n",
      "Epoch 55: Train MPJPE = 183.1318, Val MPJPE = 323.9564\n",
      "Epoch 56: Train MPJPE = 168.9648, Val MPJPE = 419.5316\n",
      "Epoch 57: Train MPJPE = 170.8634, Val MPJPE = 347.0918\n",
      "Epoch 58: Train MPJPE = 177.6791, Val MPJPE = 427.4395\n",
      "Epoch 59: Train MPJPE = 180.8803, Val MPJPE = 418.4252\n",
      "Epoch 60: Train MPJPE = 188.8221, Val MPJPE = 448.4823\n",
      "Epoch 61: Train MPJPE = 170.7777, Val MPJPE = 477.9256\n",
      "Epoch 62: Train MPJPE = 182.1396, Val MPJPE = 371.1877\n",
      "Epoch 63: Train MPJPE = 187.4438, Val MPJPE = 324.1684\n",
      "Epoch 64: Train MPJPE = 169.4473, Val MPJPE = 548.5289\n",
      "Epoch 65: Train MPJPE = 172.7663, Val MPJPE = 381.7232\n",
      "Epoch 66: Train MPJPE = 166.9099, Val MPJPE = 460.3636\n",
      "Epoch 67: Train MPJPE = 170.8789, Val MPJPE = 449.6478\n",
      "Epoch 68: Train MPJPE = 177.7850, Val MPJPE = 336.6846\n",
      "Epoch 69: Train MPJPE = 166.2699, Val MPJPE = 321.4912\n",
      "Epoch 70: Train MPJPE = 162.0276, Val MPJPE = 342.4377\n",
      "Epoch 71: Train MPJPE = 168.9423, Val MPJPE = 348.8265\n",
      "Epoch 72: Train MPJPE = 164.9793, Val MPJPE = 445.6041\n",
      "Epoch 73: Train MPJPE = 158.6168, Val MPJPE = 517.5725\n",
      "Epoch 74: Train MPJPE = 165.4562, Val MPJPE = 313.9180\n",
      "Epoch 75: Train MPJPE = 160.8092, Val MPJPE = 317.6693\n",
      "Epoch 76: Train MPJPE = 167.8061, Val MPJPE = 377.1598\n",
      "Epoch 77: Train MPJPE = 163.8982, Val MPJPE = 359.0282\n",
      "Epoch 78: Train MPJPE = 152.8855, Val MPJPE = 374.2280\n",
      "Epoch 79: Train MPJPE = 144.1984, Val MPJPE = 350.2414\n",
      "Epoch 80: Train MPJPE = 159.5710, Val MPJPE = 339.2430\n",
      "Epoch 81: Train MPJPE = 179.4049, Val MPJPE = 347.5261\n",
      "Epoch 82: Train MPJPE = 157.8763, Val MPJPE = 303.3558\n",
      "Epoch 83: Train MPJPE = 150.2125, Val MPJPE = 346.4849\n",
      "Epoch 84: Train MPJPE = 147.9307, Val MPJPE = 314.8077\n",
      "Epoch 85: Train MPJPE = 162.7155, Val MPJPE = 331.1442\n",
      "Epoch 86: Train MPJPE = 156.4631, Val MPJPE = 302.6919\n",
      "Epoch 87: Train MPJPE = 158.6353, Val MPJPE = 332.1522\n",
      "Epoch 88: Train MPJPE = 158.0008, Val MPJPE = 307.5312\n",
      "Epoch 89: Train MPJPE = 154.0238, Val MPJPE = 394.9459\n",
      "Epoch 90: Train MPJPE = 148.8071, Val MPJPE = 363.7637\n",
      "Epoch 91: Train MPJPE = 150.4611, Val MPJPE = 309.3732\n",
      "Epoch 92: Train MPJPE = 145.8268, Val MPJPE = 368.4108\n",
      "Epoch 93: Train MPJPE = 138.8408, Val MPJPE = 347.3623\n",
      "Epoch 94: Train MPJPE = 155.0719, Val MPJPE = 318.0386\n",
      "Epoch 95: Train MPJPE = 143.8011, Val MPJPE = 346.4763\n",
      "Epoch 96: Train MPJPE = 154.2525, Val MPJPE = 332.2461\n",
      "Epoch 97: Train MPJPE = 131.9934, Val MPJPE = 340.6276\n",
      "Epoch 98: Train MPJPE = 151.8980, Val MPJPE = 419.6758\n",
      "Epoch 99: Train MPJPE = 147.0406, Val MPJPE = 430.3979\n",
      "Epoch 100: Train MPJPE = 146.8491, Val MPJPE = 331.9747\n"
     ]
    }
   ],
   "source": [
    "model = CustomNet()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=best_params['lr'], weight_decay=best_params['weight_decay'])\n",
    "# criterion = torch.nn.MSELoss()\n",
    "criterion = MPJPE()\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs = [inp.float() for inp in inputs]\n",
    "        targets = targets.float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * targets.size(0)\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = [inp.float() for inp in inputs]\n",
    "            targets = targets.float()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item() * targets.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train MPJPE = {avg_train_loss:.4f}, Val MPJPE = {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d14e160c-abb0-499a-9704-1bf0d5b2fc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss = 133.9710\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item() * targets.size(0)\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print(f\"Test loss = {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cc4859f1-23f8-4258-88f1-52e73fe1f6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(model, inputs_list, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    inputs = [torch.from_numpy(inp).float().unsqueeze(0).to(device) for inp in inputs_list]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(inputs) \n",
    "    \n",
    "    return output.squeeze(0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "05504998-252d-43f4-a2de-dc1094ee3dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8.542352676391602, 3.7042770385742188, 0.4601781368255615],\n",
       " [8.733415603637695, 2.050076961517334, 0.3684786558151245],\n",
       " [8.922904014587402, 0.40957820415496826, 0.27750110626220703],\n",
       " [8.619648933410645, 3.694295883178711, -0.6643123030662537],\n",
       " [8.59278678894043, 2.0004396438598633, -0.4504869282245636],\n",
       " [8.83590316772461, 0.44832444190979004, -0.34511709213256836],\n",
       " [8.306090354919434, 5.630198001861572, 0.5446023941040039],\n",
       " [8.643503189086914, 4.524703025817871, 0.8668829202651978],\n",
       " [8.496593475341797, 3.8861234188079834, 0.7753106355667114],\n",
       " [8.430327415466309, 5.595487594604492, -0.6955798864364624],\n",
       " [8.854900360107422, 4.437718391418457, -0.8193653225898743],\n",
       " [8.724615097045898, 3.779153823852539, -0.794681966304779]]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(raw_output[test_seq_set[0]][0].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "688109ed-9ebc-45a3-8ae6-28968d7d4588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p19s1\n",
      "[[-7.520870685577393, 3.7407069206237793, 0.9504961967468262], [-7.368579387664795, 2.096409797668457, 0.7714738249778748], [-7.216275691986084, 0.45193934440612793, 0.5923908352851868], [-7.585234642028809, 3.6390230655670166, -0.24816995859146118], [-8.271810531616211, 2.0671019554138184, -0.2194407731294632], [-8.13848876953125, 0.6275912523269653, -0.049029216170310974], [-7.470393657684326, 5.788812160491943, 0.9987093806266785], [-7.426072120666504, 4.562010765075684, 1.3400733470916748], [-7.77842903137207, 3.962614059448242, 1.3499772548675537], [-7.44651985168457, 5.737953186035156, -0.41605842113494873], [-7.228679656982422, 4.530491828918457, -0.6374579668045044], [-7.36409330368042, 3.844463348388672, -0.6903193593025208]]\n",
      "\n",
      "[[170.03086081600878, -1844.6219957094079, 130.88436771912143], [17.02853537503636, -1944.996260944025, 163.4218074608411], [186.44276651431238, -1878.2360880658093, 503.01631521967727], [-41.32293539639176, -2011.1317421384556, 523.3997112667856], [186.82897075793994, -1855.5908332828958, 883.5462743010441], [-1.045497397891326, -1857.614676477385, 884.1471920351526], [271.6337277528243, -1818.865894737878, 1411.436565627421], [-86.32199341688383, -1803.7583307411094, 1412.9511780833614]]\n"
     ]
    }
   ],
   "source": [
    "test_seq = test_seq_set[2]\n",
    "print(test_seq)\n",
    "frame = 90\n",
    "\n",
    "bvh_sample_data = list(raw_output[test_seq][frame].values())\n",
    "triangulation_sample_all_data = triangulation_data[test_seq][frame]\n",
    "triangulation_sample_data = [triangulation_sample_all_data[int(j_idx)] for j_idx in selected_names.keys()]\n",
    "print(bvh_sample_data)\n",
    "print()\n",
    "print(triangulation_sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "73cf4ecc-a65d-4e20-99f3-e508e82cb239",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.20789899, 0.7136336 ],\n",
       "        [0.20637479, 0.68947649],\n",
       "        [0.19463433, 0.603342  ],\n",
       "        [0.19536977, 0.59146768],\n",
       "        [0.18979931, 0.49922755],\n",
       "        [0.20980054, 0.49864772],\n",
       "        [0.17801417, 0.34159309],\n",
       "        [0.21851808, 0.343532  ]]),\n",
       " array([[0.50281024, 0.52718848],\n",
       "        [0.51541841, 0.52079028],\n",
       "        [0.50254595, 0.47092509],\n",
       "        [0.51958108, 0.46615925],\n",
       "        [0.50304776, 0.41416296],\n",
       "        [0.51714069, 0.41404751],\n",
       "        [0.49629128, 0.33432806],\n",
       "        [0.52493501, 0.33382931]]),\n",
       " array([[0.77794898, 0.65248728],\n",
       "        [0.80610824, 0.65606916],\n",
       "        [0.7854656 , 0.55615252],\n",
       "        [0.82918954, 0.56113464],\n",
       "        [0.78692073, 0.45205042],\n",
       "        [0.80421245, 0.45428443],\n",
       "        [0.78226632, 0.3010366 ],\n",
       "        [0.81265831, 0.30195358]]),\n",
       " array([[0.52748507, 0.60908717],\n",
       "        [0.50190574, 0.61400777],\n",
       "        [0.53221762, 0.51364052],\n",
       "        [0.49170411, 0.51178759],\n",
       "        [0.53315943, 0.38555178],\n",
       "        [0.4975619 , 0.38328782],\n",
       "        [0.55019039, 0.20827766],\n",
       "        [0.48714897, 0.20370464]])]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_width = 960\n",
    "img_height = 540\n",
    "\n",
    "mp_input_sample = []\n",
    "\n",
    "for c_idx in range(1, 5):\n",
    "    all_frames_for_camera = raw_input[test_seq][f\"c{c_idx}\"][str(frame)]\n",
    "    camera_mp_input_sample = []\n",
    "    \n",
    "    for point_idx, joint_name in selected_names.items(): \n",
    "        pixel_coords = all_frames_for_camera[int(point_idx)]\n",
    "        camera_mp_input_sample.append(pixel_coords)\n",
    "        # camera_mp_input_sample.append([pixel_coords[0]/img_width, pixel_coords[1]/img_height])\n",
    "\n",
    "    mp_input_sample.append(np.array(camera_mp_input_sample))\n",
    "    \n",
    "mp_input_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1a4b9737-6623-4a1e-be25-d3dd476309e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1683.3273  ,   155.53522 ,   170.442   ],\n",
       "       [-1914.3871  ,   123.715096,    38.835804],\n",
       "       [-1839.3185  ,   531.7227  ,   215.95714 ],\n",
       "       [-1953.0719  ,   525.3908  ,    -8.404432],\n",
       "       [-1834.1058  ,   943.9152  ,   252.93167 ],\n",
       "       [-1835.9868  ,   941.33215 ,   -62.155464],\n",
       "       [-1819.9902  ,  1415.7582  ,   306.61847 ],\n",
       "       [-1802.6725  ,  1410.7279  ,   -97.65706 ]], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = predict_single(model, mp_input_sample, 'cpu')\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3d96e453-d8b5-4321-8409-a2d47635d0e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"820px\"\n",
       "    height=\"820\"\n",
       "    src=\"iframe_figures/figure_86.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe'\n",
    "\n",
    "SCALE_FACTOR = 255\n",
    "\n",
    "x = [vec[2]*SCALE_FACTOR for vec in bvh_sample_data]\n",
    "y = [vec[0]*SCALE_FACTOR for vec in bvh_sample_data]\n",
    "z = [vec[1]*SCALE_FACTOR for vec in bvh_sample_data]\n",
    "\n",
    "# x_t = [vec[0]/SCALE_FACTOR for vec in triangulation_sample_data]\n",
    "# y_t = [vec[1]/SCALE_FACTOR for vec in triangulation_sample_data]\n",
    "# z_t = [vec[2]/SCALE_FACTOR for vec in triangulation_sample_data]\n",
    "\n",
    "x_t = [vec[0] for vec in triangulation_sample_data]\n",
    "y_t = [vec[1] for vec in triangulation_sample_data]\n",
    "z_t = [vec[2] for vec in triangulation_sample_data]\n",
    "\n",
    "x_p = [vec[2] for vec in predicted]\n",
    "y_p = [vec[0] for vec in predicted]\n",
    "z_p = [vec[1] for vec in predicted]\n",
    "    \n",
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Scatter3d(\n",
    "            x=x, y=y, z=z,\n",
    "            mode='markers',\n",
    "            marker=dict(size=5, color='blue'),\n",
    "            hoverinfo='text',\n",
    "            name='Joints BVH'),\n",
    "        go.Scatter3d(\n",
    "            x=x_t, y=y_t, z=z_t,\n",
    "            mode='markers',\n",
    "            marker=dict(size=5, color='red'),\n",
    "            hoverinfo='text',\n",
    "            name='Joints triangulation mediapipe'),\n",
    "        go.Scatter3d(\n",
    "            x=x_p, y=y_p, z=z_p,\n",
    "            mode='markers',\n",
    "            marker=dict(size=5, color='green'),\n",
    "            hoverinfo='text',\n",
    "            name='Predicted by NN'),\n",
    "        ]\n",
    ")\n",
    "\n",
    "fig.update_layout(scene=dict(\n",
    "    xaxis_title='X',\n",
    "    yaxis_title='Y',\n",
    "    zaxis_title='Z',\n",
    "    xaxis=dict(range=[-6000, 6000]),\n",
    "    yaxis=dict(range=[-6000, 6000]),\n",
    "    zaxis=dict(range=[-6000, 6000]),\n",
    "    aspectmode='cube', \n",
    "),\n",
    "title='3D joints plot from bvh file',\n",
    "width=800,\n",
    "height=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db610651-515e-437b-aaf3-b9c3e295777b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
